{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "df_train= pd.read_csv(\"https://raw.githubusercontent.com/alexerne-git/ML-Zoom/master/Data/training_data.csv\")\n",
    "df_test = pd.read_csv(\"https://raw.githubusercontent.com/alexerne-git/ML-Zoom/master/Data/test_data.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An aventure in the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to understand a little more about our data.\n",
    "\n",
    "Distribution, similarity between the test set and the train set, missing values and other stuff.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Location"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot NaN values to see the distribution\n",
    "\n",
    "NAn= len(df_train.location[df_train[\"location\"].isnull()==True])\n",
    "Val= len(df_train.location)-NAn\n",
    "#Piechart\n",
    "labels = 'Nan', 'Values'\n",
    "sizes = [NAn, Val]\n",
    "colors = ['gold', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "NAn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NAn= len(df_test.location[df_test[\"location\"].isnull()==True])\n",
    "Val= len(df_test.location)-NAn\n",
    "#Piechart\n",
    "labels = 'Nan', 'Values'\n",
    "sizes = [NAn, Val]\n",
    "colors = ['gold', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A lot of Missing values. Maybe we need to drop it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some text Distribution\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# word_count\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "\n",
    "# url_count\n",
    "df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "# mean_word_length\n",
    "df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# char_count\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "df_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# punctuation_count\n",
    "df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# hashtag_count\n",
    "df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "# mention_count\n",
    "df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distribution Graphics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Graphic distribution in test & train data for mention count \n",
    "sns.set_style(\"white\")\n",
    "x1 = df_train[\"mention_count\"]\n",
    "x2 = df_test[\"mention_count\"]\n",
    "\n",
    "\n",
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.6}, kde_kws={'linewidth':2})\n",
    "\n",
    "plt.figure(figsize=(10,7), dpi= 80)\n",
    "sns.distplot(x1, color=\"dodgerblue\", label=\"Train\", **kwargs)\n",
    "sns.distplot(x2, color=\"orange\", label=\"Test\", **kwargs)\n",
    "plt.xlim(0,5)\n",
    "plt.legend();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graphic distribution in test & train data for hashtag count\n",
    "sns.set_style(\"white\")\n",
    "x1 = df_train[\"hashtag_count\"]\n",
    "x2 = df_test[\"hashtag_count\"]\n",
    "\n",
    "\n",
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.6}, kde_kws={'linewidth':2})\n",
    "\n",
    "plt.figure(figsize=(10,7), dpi= 80)\n",
    "sns.distplot(x1, color=\"dodgerblue\", label=\"Train\", **kwargs)\n",
    "sns.distplot(x2, color=\"orange\", label=\"Test\", **kwargs)\n",
    "plt.xlim(0,5)\n",
    "plt.legend();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graphic distribution in test & train data for url count\n",
    "sns.set_style(\"white\")\n",
    "x1 = df_train[\"url_count\"]\n",
    "x2 = df_test[\"url_count\"]\n",
    "\n",
    "\n",
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.6}, kde_kws={'linewidth':2})\n",
    "\n",
    "plt.figure(figsize=(10,7), dpi= 80)\n",
    "sns.distplot(x1, color=\"dodgerblue\", label=\"Train\", **kwargs)\n",
    "sns.distplot(x2, color=\"orange\", label=\"Test\", **kwargs)\n",
    "plt.xlim(0,5)\n",
    "plt.legend();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graphic distribution in test & train data for punctuation count\n",
    "\n",
    "x1 = df_train[\"punctuation_count\"]\n",
    "x2 = df_test[\"punctuation_count\"]\n",
    "\n",
    "\n",
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.6}, kde_kws={'linewidth':2})\n",
    "\n",
    "plt.figure(figsize=(10,7), dpi= 80)\n",
    "sns.distplot(x1, color=\"dodgerblue\", label=\"Train\", **kwargs)\n",
    "sns.distplot(x2, color=\"orange\", label=\"Test\", **kwargs)\n",
    "plt.xlim(0,30)\n",
    "plt.legend();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "x1 = df_train.loc[df_train.target==1,\"hashtag_count\"]\n",
    "x2 = df_train.loc[df_train.target==0, \"hashtag_count\"]\n",
    "\n",
    "\n",
    "# Plot\n",
    "kwargs = dict(hist_kws={'alpha':.6}, kde_kws={'linewidth':2})\n",
    "\n",
    "plt.figure(figsize=(10,7), dpi= 80)\n",
    "sns.distplot(x1, color=\"dodgerblue\", label=\"Disaster\", **kwargs)\n",
    "sns.distplot(x2, color=\"orange\", label=\"No Disaster\", **kwargs)\n",
    "plt.xlim(0,6)\n",
    "plt.legend();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ngrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unigrams\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Distribution of every words in the train dataset \n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import spacy \n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "words=[]\n",
    "for t in df_train.text:\n",
    "  for word in t.split():\n",
    "      words.append(word)\n",
    "x=(pd.Series(nltk.ngrams(words, 1)).value_counts())[:50]\n",
    "\n",
    "x.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target value can affect the unigrams ?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_answer2=df_train[df_train[\"target\"]==0]\n",
    "words=[]\n",
    "for t in df_answer2.text:\n",
    "  for word in t.split():\n",
    "      words.append(word)\n",
    "x2=(pd.Series(nltk.ngrams(words, 1)).value_counts())[:50]\n",
    "x2.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Distribution des mots: \n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import spacy \n",
    "\n",
    "#Disaster \n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "df_answer=df_train[df_train[\"target\"]==1]\n",
    "words=[]\n",
    "for t in df_answer.text:\n",
    "  for word in t.split():\n",
    "      words.append(word)\n",
    "x=(pd.Series(nltk.ngrams(words, 1)).value_counts())[:50]\n",
    "x.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bigram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words=[]\n",
    "for t in df_train.text:\n",
    "  for word in t.split():\n",
    "      words.append(word)\n",
    "x=(pd.Series(nltk.ngrams(words, 2)).value_counts())[:50]\n",
    "\n",
    "x.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words=[]\n",
    "for t in df_train.text:\n",
    "  for word in t.split():\n",
    "      words.append(word)\n",
    "x=(pd.Series(nltk.ngrams(words, 3)).value_counts())[:50]\n",
    "\n",
    "x.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to clean some data as &amp or the MH370 case.\n",
    "Maybe say something for the distribution of words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keywords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keyw=[]\n",
    "for i in df_train.keyword:\n",
    "  keyw.append(i)\n",
    "counted=Counter(keyw)\n",
    "x=counted.most_common(150)\n",
    "#Key\n",
    "key=[]\n",
    "for i in range(len(x)):\n",
    "  key.append(x[i][0])\n",
    "\n",
    "\n",
    "key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a problem with the keywords.The space isn't decoded correctly. We can see in the text some %20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train[\"keyword\"]= df_train[\"keyword\"].astype(str)\n",
    "for i in range(len(df_train[\"keyword\"])):\n",
    "  df_train[\"keyword\"].iloc[i]=df_train[\"keyword\"].iloc[i].replace(\"%20\",\" \")\n",
    "\n",
    "\n",
    "df_test[\"keyword\"]= df_test[\"keyword\"].astype(str)\n",
    "for i in range(len(df_test[\"keyword\"])):\n",
    "  df_test[\"keyword\"].iloc[i]=df_test[\"keyword\"].iloc[i].replace(\"%20\",\" \")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train\n",
    "keyw=[]\n",
    "for i in df_train.keyword:\n",
    "  keyw.append(i)\n",
    "counted=Counter(keyw)\n",
    "x=counted.most_common(15)\n",
    "#Key\n",
    "key=[]\n",
    "for i in range(len(x)):\n",
    "  key.append(x[i][0])\n",
    "\n",
    "#Values:\n",
    "values=[]\n",
    "for i in range(len(x)):\n",
    "  values.append(x[i][1])\n",
    "values\n",
    "data = {'Key': key,\n",
    "        'Values': values\n",
    "       }\n",
    "df = pd.DataFrame(data,columns=['Key','Values'])\n",
    "df.plot(x ='Key', y='Values', kind = 'bar')\n",
    "plt.title(\"Keyword's Distribution\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test\n",
    "keyw=[]\n",
    "for i in df_test.keyword:\n",
    "  keyw.append(i)\n",
    "counted=Counter(keyw)\n",
    "x=counted.most_common(15)\n",
    "#Key\n",
    "key=[]\n",
    "for i in range(len(x)):\n",
    "  key.append(x[i][0])\n",
    "\n",
    "#Values:\n",
    "values=[]\n",
    "for i in range(len(x)):\n",
    "  values.append(x[i][1])\n",
    "values\n",
    "data = {'Key': key,\n",
    "        'Values': values\n",
    "       }\n",
    "df = pd.DataFrame(data,columns=['Key','Values'])\n",
    "df.plot(x ='Key', y='Values', kind = 'bar')\n",
    "plt.title(\"Keyword's Distribution\")\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see Nan in the train set but don't appear on the test set. We are going to check this deeper:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check different distribution of Nan keywords in the different datasets (Test and Train)\n",
    "\n",
    "Missing_test= df_test.keyword[df_test[\"keyword\"]==\"nan\"].count()\n",
    "Missing_train=df_train.keyword[df_train[\"keyword\"]==\"nan\"].count()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = [\"Test\", \"Train\"]\n",
    "students = [Missing_train,Missing_test]\n",
    "ax.bar(langs,students)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the distribution of Disaster / Not disaster in the training set \n",
    "Disaster= df_train.target[df_train[\"target\"]==1].count()\n",
    "NoDisaster= df_train.target[df_train[\"target\"]==0].count()\n",
    "#Piechart\n",
    "labels = 'Disaster', 'Not Disaster'\n",
    "sizes = [Disaster, NoDisaster]\n",
    "colors = ['gold', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus WordCloud"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WordCloud to visualise occurence of words in the training set where target = 0\n",
    "\n",
    "no_dis_tweets = df_train[df_train.target == 0]\n",
    "word = []\n",
    "for t in no_dis_tweets.text:\n",
    "    word.append(t)\n",
    "word = pd.Series(word).str.cat(sep=' ')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(word)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WordCloud to visualise occurence of words in the training set where target = 1\n",
    "\n",
    "no_dis_tweets = df_train[df_train.target == 1]\n",
    "word = []\n",
    "for t in no_dis_tweets.text:\n",
    "    word.append(t)\n",
    "word = pd.Series(word).str.cat(sep=' ')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(word)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML PRoject v.2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}